



##################################################################################################




KMEANS 

# Import KMeans

from sklearn.cluster import KMeans
        
# Create a KMeans instance with 3 clusters: model
model = KMeans(n_clusters=3)
        
# Fit model to points
model.fit(points)
        
# Determine the cluster labels of new_points: labels
labels = model.predict(new_points)
    
# Print cluster labels of new_points
print(labels)
[1 2 0 1 2 1 2 2 2 0 1 2 2 0 0 2 0 0 2 2 0 2 1 2 1 0 2 0 0 1 1 2 2 2 0 1 2
 2 1 2 0 1 1 0 1 2 0 0 2 2 2 2 0 0 1 1 0 0 0 1 1 2 2 2 1 2 0 2 1 0 1 1 1 2
 1 0 0 1 2 0 1 0 1 2 0 2 0 1 2 2 2 1 2 2 1 0 0 0 0 1 2 1 0 0 1 1 2 1 0 0 1
 0 0 0 2 2 2 2 0 0 2 1 2 0 2 1 0 2 0 0 2 0 2 0 1 2 1 1 2 0 1 2 1 1 0 2 2 1
 0 1 0 2 1 0 0 1 0 2 2 0 2 0 0 2 2 1 2 2 0 1 0 1 1 2 1 2 2 1 1 0 1 1 1 0 2
 2 1 0 1 0 0 2 2 2 1 2 2 2 0 0 1 2 1 1 1 0 2 2 2 2 2 2 0 0 2 0 0 0 0 2 0 0
 2 2 1 0 1 1 0 1 0 1 0 2 2 0 2 2 2 0 1 1 0 2 2 0 2 0 0 2 0 0 1 0 1 1 1 2 0
 0 0 1 2 1 0 1 0 0 2 1 1 1 0 2 2 2 1 2 0 0 2 1 1 0 1 1 0 1 2 1 0 0 0 0 2 0
 0 2 2 1]





##################################################################################################





KMEANS


# Import pyplot
from matplotlib import pyplot as plt

# Assign the columns of new_points: xs and ys
xs = new_points[:,0]
ys = new_points[:,1]

# Make a scatter plot of xs and ys, using labels to define the colors
plt.scatter(xs, ys, c=labels, alpha=0.5)


# Assign the cluster centers: centroids
centroids = model.cluster_centers_
# these will be the two dimensional representation of the centroids


# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]



# Make a scatter plot of centroids_x and centroids_y
plt.scatter(centroids_x, centroids_y, marker='D', s=50)
plt.show()




print(centroids):
[[-1.57568905 -0.22531944]
 [ 0.18034887 -0.81701955]
 [ 1.01378685  0.98288627]]






##################################################################################################






ELBOW PLOT



ks = range(1, 6)
inertias = []

for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(samples)
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
# Plot ks vs inertias
plt.plot(ks, inertias, '-o')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()






##################################################################################################






# Create a KMeans model with 3 clusters: model
model = KMeans(n_clusters=3)

# Use fit_predict to fit model and obtain cluster labels: labels
labels = model.fit_predict(samples)

# Create a DataFrame with clusters and varieties as columns: df
df = pd.DataFrame({'labels': labels, 'varieties': varieties})
# I WANT TO HAVE:  the labels, and the varieties 

# Create crosstab: ct
ct = pd.crosstab(df['labels'], df['varieties'])

# Display ct
print(ct)


# crosstab:    
print(ct)

varieties  Canadian wheat  Kama wheat  Rosa wheat
labels                                           
0                       0           1          60
1                      68           9           0
2                       2          60          10






##################################################################################################




SCALE




print(dir(pipeline)): 

'classes_', 'decision_function', 'fit', 'fit_predict', 'fit_transform', 'get_params', 
'inverse_transform', 'memory', 'named_steps', 'predict', 'predict_log_proba', 
'predict_proba', 'score', 'set_params', 'steps', 'transform', 'verbose'



# Perform the necessary imports
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Create scaler: scaler
scaler = StandardScaler()

# Create KMeans instance: kmeans
kmeans = KMeans(n_clusters=4)

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, kmeans)

print(dir(pipeline))




print(kmeans.n_clusters)
4






##################################################################################################






# Import pandas
import pandas as pd

# Fit the pipeline to samples
pipeline.fit(samples)

# Calculate the cluster labels: labels
labels = pipeline.predict(samples)

# Create a DataFrame with labels and species as columns: df
df = pd.DataFrame({'labels': labels, 'species': species})

# Create crosstab: ct
ct = pd.crosstab(df['labels'], df['species'])

# Display ct
print(ct)



print(ct)
species  Bream  Pike  Roach  Smelt
labels                            
0            0     0      0     13
1           33     0      1      0
2            0    17      0      0
3            1     0     19      1










# Import Normalizer
from sklearn.preprocessing import Normalizer

# Create a normalizer: normalizer
normalizer = Normalizer()

# Create a KMeans model with 10 clusters: kmeans
kmeans = KMeans(n_clusters=10)

# Make a pipeline chaining normalizer and kmeans: pipeline
pipeline = make_pipeline(normalizer, kmeans)

# Fit pipeline to the daily price movements
pipeline.fit(movements)


Out[1]: 
Pipeline(memory=None,
         steps=[('normalizer', Normalizer(copy=True, norm='l2')),
                ('kmeans',
                 KMeans(algorithm='auto', copy_x=True, init='k-means++',
                        max_iter=300, n_clusters=10, n_init=10, n_jobs=None,
                        precompute_distances='auto', random_state=None,
                        tol=0.0001, verbose=0))],
         verbose=False)









##################################################################################################







# Import pandas
import pandas as pd

# Predict the cluster labels: labels
labels = pipeline.predict(movements)

# Create a DataFrame aligning labels and companies: df
df = pd.DataFrame({'labels': labels, 'companies': companies})

# Display df sorted by cluster label
print(df.sort_values('labels'))




print(df.sort_values('labels'))

    labels                           companies
59       0                               Yahoo
15       0                                Ford
35       0                            Navistar
26       1                      JPMorgan Chase
16       1                   General Electrics
58       1                               Xerox
11       1                               Cisco
18       1                       Goldman Sachs
20       1                          Home Depot
5        1                     Bank of America
3        1                    American express
55       1                         Wells Fargo
1        1                                 AIG
38       2                               Pepsi
40       2                      Procter Gamble
28       2                           Coca Cola
27       2                      Kimberly-Clark
9        2                   Colgate-Palmolive
54       3                            Walgreen
36       3                    Northrop Grumman
29       3                     Lookheed Martin
4        3                              Boeing
0        4                               Apple
47       4                            Symantec
33       4                           Microsoft
32       4                                  3M
31       4                           McDonalds
30       4                          MasterCard
50       4  Taiwan Semiconductor Manufacturing
14       4                                Dell
17       4                     Google/Alphabet
24       4                               Intel
23       4                                 IBM
2        4                              Amazon
51       4                   Texas instruments
43       4                                 SAP
45       5                                Sony
48       5                              Toyota
21       5                               Honda
22       5                                  HP
34       5                          Mitsubishi
7        5                               Canon
56       6                            Wal-Mart
57       7                               Exxon
44       7                        Schlumberger
8        7                         Caterpillar
10       7                      ConocoPhillips
12       7                             Chevron
13       7                   DuPont de Nemours
53       7                       Valero Energy
39       8                              Pfizer
41       8                       Philip Morris
25       8                   Johnson & Johnson
49       9                               Total
46       9                      Sanofi-Aventis
37       9                            Novartis
42       9                   Royal Dutch Shell
19       9                     GlaxoSmithKline
52       9                            Unilever
6        9            British American Tobacco







##################################################################################################




In [1]: # Perform the necessary imports
        from scipy.cluster.hierarchy import linkage, dendrogram
        import matplotlib.pyplot as plt
        
        # Calculate the linkage: mergings
        mergings = linkage(samples, method='complete')
        
        # Plot the dendrogram, using varieties as labels
        dendrogram(mergings,
                   labels=varieties,
                   leaf_rotation=90,
                   leaf_font_size=6,
        )
        plt.show()
        
        
        print("Varieties:  ")
        print(varieties)
        
        print("Samples:")
        print(samples)


Out:


Varieties:  
['Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat']





Samples:
[[14.88   14.57    0.8811  5.554   3.333   1.018   4.956 ]
 [14.69   14.49    0.8799  5.563   3.259   3.586   5.219 ]
 [14.03   14.16    0.8796  5.438   3.201   1.717   5.001 ]
 [13.99   13.83    0.9183  5.119   3.383   5.234   4.781 ]
 [14.11   14.26    0.8722  5.52    3.168   2.688   5.219 ]
 [13.02   13.76    0.8641  5.395   3.026   3.373   4.825 ]
 [15.49   14.94    0.8724  5.757   3.371   3.412   5.228 ]
 [16.2    15.27    0.8734  5.826   3.464   2.823   5.527 ]
 [13.5    13.85    0.8852  5.351   3.158   2.249   5.176 ]
 [15.36   14.76    0.8861  5.701   3.393   1.367   5.132 ]
 [15.78   14.91    0.8923  5.674   3.434   5.593   5.136 ]
 [14.46   14.35    0.8818  5.388   3.377   2.802   5.044 ]
 [11.23   12.63    0.884   4.902   2.879   2.269   4.703 ]
 [14.34   14.37    0.8726  5.63    3.19    1.313   5.15  ]
 [16.84   15.67    0.8623  5.998   3.484   4.675   5.877 ]
 [17.32   15.91    0.8599  6.064   3.403   3.824   5.922 ]
 [18.72   16.19    0.8977  6.006   3.857   5.324   5.879 ]
 [18.88   16.26    0.8969  6.084   3.764   1.649   6.109 ]
 [18.76   16.2     0.8984  6.172   3.796   3.12    6.053 ]
 [19.31   16.59    0.8815  6.341   3.81    3.477   6.238 ]
 [17.99   15.86    0.8992  5.89    3.694   2.068   5.837 ]
 [18.85   16.17    0.9056  6.152   3.806   2.843   6.2   ]
 [19.38   16.72    0.8716  6.303   3.791   3.678   5.965 ]
 [18.96   16.2     0.9077  6.051   3.897   4.334   5.75  ]
 [18.14   16.12    0.8772  6.059   3.563   3.619   6.011 ]
 [18.65   16.41    0.8698  6.285   3.594   4.391   6.102 ]
 [18.94   16.32    0.8942  6.144   3.825   2.908   5.949 ]
 [17.36   15.76    0.8785  6.145   3.574   3.526   5.971 ]
 [13.32   13.94    0.8613  5.541   3.073   7.035   5.44  ]
 [11.43   13.13    0.8335  5.176   2.719   2.221   5.132 ]
 [12.01   13.52    0.8249  5.405   2.776   6.992   5.27  ]
 [11.34   12.87    0.8596  5.053   2.849   3.347   5.003 ]
 [12.02   13.33    0.8503  5.35    2.81    4.271   5.308 ]
 [12.44   13.59    0.8462  5.319   2.897   4.924   5.27  ]
 [11.55   13.1     0.8455  5.167   2.845   6.715   4.956 ]
 [11.26   13.01    0.8355  5.186   2.71    5.335   5.092 ]
 [12.46   13.41    0.8706  5.236   3.017   4.987   5.147 ]
 [11.81   13.45    0.8198  5.413   2.716   4.898   5.352 ]
 [11.27   12.86    0.8563  5.091   2.804   3.985   5.001 ]
 [12.79   13.53    0.8786  5.224   3.054   5.483   4.958 ]
 [12.67   13.32    0.8977  4.984   3.135   2.3     4.745 ]
 [11.23   12.88    0.8511  5.14    2.795   4.325   5.003 ]]






Mergings:
[[33.         36.          0.27162909  2.        ]
 [21.         26.          0.31365739  2.        ]
 [18.         43.          0.32846589  3.        ]
 [38.         41.          0.34657328  2.        ]
 [19.         22.          0.37233454  2.        ]
 [15.         27.          0.38916958  2.        ]
 [ 4.         11.          0.48519909  2.        ]
 [ 2.         13.          0.60220511  2.        ]
 [23.         25.          0.64447995  2.        ]
 [ 0.          9.          0.66671658  2.        ]
 [32.         37.          0.68359363  2.        ]
 [39.         42.          0.75541297  3.        ]
 [12.         29.          0.76129577  2.        ]
 [30.         34.          0.79066703  2.        ]
 [24.         47.          0.89015184  3.        ]
 [ 1.          6.          0.96077742  2.        ]
 [31.         45.          0.98956619  3.        ]
 [16.         50.          1.05891757  3.        ]
 [17.         20.          1.11543099  2.        ]
 [ 8.         40.          1.13733735  2.        ]
 [44.         46.          1.1662041   5.        ]
 [ 5.         61.          1.28676337  3.        ]
 [35.         52.          1.37690488  3.        ]
 [48.         49.          1.52865125  4.        ]
 [53.         64.          1.66517195  6.        ]
 [14.         56.          1.74234784  4.        ]
 [51.         65.          1.91015424  6.        ]
 [ 7.         57.          1.91749035  3.        ]
 [28.         55.          2.08980038  3.        ]
 [54.         58.          2.13385537  5.        ]
 [ 3.         10.          2.22187038  2.        ]
 [59.         67.          2.31852251  7.        ]
 [60.         62.          2.33686195  7.        ]
 [68.         69.          2.76779035  9.        ]
 [66.         70.          3.13448417  9.        ]
 [63.         71.          3.25744652  8.        ]
 [73.         74.          3.71580316 14.        ]
 [72.         75.          4.68116988 11.        ]
 [76.         77.          5.45789312 17.        ]
 [78.         79.          6.74608427 25.        ]
 [80.         81.          9.61230238 42.        ]]




















##################################################################################################






In chapter 1, you used k-means clustering to cluster companies according to their stock price movements. Now, you'll perform hierarchical clustering of the companies. You are given a NumPy array of price movements movements, where the rows correspond to companies, and a list of the company names companies. SciPy hierarchical clustering doesn't fit into a sklearn pipeline, so you'll need to use the normalize() function from sklearn.preprocessing instead of Normalizer.

linkage and dendrogram have already been imported from scipy.cluster.hierarchy, and PyPlot has been imported as plt.

Instructions
0 XP
Import normalize from sklearn.preprocessing.
Rescale the price movements for each stock by using the normalize() function on movements.
Apply the linkage() function to normalized_movements, using 'complete' linkage, to calculate the hierarchical clustering. Assign the result to mergings.
Plot a dendrogram of the hierarchical clustering, using the list companies of company names as the labels. In addition, specify the leaf_rotation=90, and leaf_font_size=6 keyword arguments as you did in the previous exercise.
Hint
To import x from y, use the command from y import x.
To rescale the price movements, use normalize(movements).
Pass in normalized_movements and method='complete' as arguments to linkage() to perform the hierarchical clustering.
To plot the dendrogram, you need to specify the following arguments inside dendrogram(): mergings, labels=companies, leaf_rotation=90, and leaf_font_size=6.

Did you find this hint helpful?








# Import normalize
from sklearn.preprocessing import normalize

# Normalize the movements: normalized_movements
normalized_movements = normalize(movements)

# Calculate the linkage: mergings
mergings = linkage(normalized_movements, method='complete')

# Plot the dendrogram
dendrogram(
    mergings,
    labels=companies,
    leaf_rotation=90,
    leaf_font_size=6
)
plt.show()





In [1]: # Import normalize
        from sklearn.preprocessing import normalize
        
        # Normalize the movements: normalized_movements
        normalized_movements = normalize(movements)
        
        # Calculate the linkage: mergings
        mergings = linkage(normalized_movements, method='complete')
        
        # Plot the dendrogram
        dendrogram(
            mergings,
            labels=companies,
            leaf_rotation=90,
            leaf_font_size=6
        )
        plt.show()

In [2]: # Import normalize
        from sklearn.preprocessing import normalize
        
        # Normalize the movements: normalized_movements
        normalized_movements = normalize(movements)
        
        print("Normalized Movements:")
        print(normalized_movements)
        
        
        # Calculate the linkage: mergings
        mergings = linkage(normalized_movements, method='complete')
        
        print("Mergings:")
        print(mergings)
        
        
        # Plot the dendrogram
        dendrogram(
            mergings,
            labels=companies,
            leaf_rotation=90,
            leaf_font_size=6
        )
        plt.show()
Normalized Movements:
[[ 0.00302051 -0.00114574 -0.01775851 ... -0.02791349  0.00437463
  -0.10202026]
 [-0.02599391 -0.02639998 -0.00852927 ... -0.00162466 -0.01624623
   0.02680614]
 [-0.02208986  0.01184398 -0.02208986 ...  0.04502568 -0.01654394
   0.03515588]
 ...
 [ 0.01981027  0.01059598  0.02626006 ... -0.01197837  0.01842816
   0.02211388]
 [ 0.0200991   0.00223323 -0.01786587 ... -0.0066997   0.00446647
  -0.0066997 ]
 [ 0.01796837  0.00112314  0.         ... -0.00673829  0.02919855
   0.01123007]]
Mergings:
[[ 42.          49.           0.58499795   2.        ]
 [ 12.          57.           0.67355098   2.        ]
 [  5.          26.           0.69371159   2.        ]
 [ 21.          48.           0.75062554   2.        ]
 [ 52.          60.           0.78536856   3.        ]
 [ 55.          62.           0.789767     3.        ]
 [ 10.          61.           0.798409     3.        ]
 [ 29.          36.           0.80409137   2.        ]
 [ 46.          64.           0.84275997   4.        ]
 [  7.          45.           0.8506084    2.        ]
 [  8.          13.           0.85283926   2.        ]
 [ 19.          37.           0.85471765   2.        ]
 [ 32.          70.           0.87300001   3.        ]
 [ 24.          51.           0.8766151    2.        ]
 [ 44.          66.           0.87947223   4.        ]
 [ 43.          68.           0.8815919    5.        ]
 [ 34.          63.           0.88949572   3.        ]
 [ 18.          65.           0.90608043   4.        ]
 [  6.          71.           0.91712927   3.        ]
 [  9.          40.           0.92637377   2.        ]
 [ 28.          38.           0.95164161   2.        ]
 [  4.          67.           0.95294663   3.        ]
 [ 16.          72.           0.9595548    4.        ]
 [ 69.          76.           0.96158158   5.        ]
 [ 11.          33.           0.97566427   2.        ]
 [ 27.          79.           0.97740201   3.        ]
 [ 23.          82.           0.99331399   5.        ]
 [  3.          77.           0.99786911   5.        ]
 [ 25.          39.           1.00141064   2.        ]
 [ 50.          73.           1.00524963   3.        ]
 [ 75.          78.           1.01130797   8.        ]
 [ 15.          83.           1.03345971   6.        ]
 [ 14.          22.           1.04058548   2.        ]
 [ 58.          74.           1.04108743   5.        ]
 [ 47.          84.           1.04435325   3.        ]
 [ 87.          91.           1.05365594  11.        ]
 [ 86.          93.           1.0581736   10.        ]
 [  2.          17.           1.06751133   2.        ]
 [ 80.          85.           1.08256519   5.        ]
 [ 88.          90.           1.09035689  10.        ]
 [ 92.          94.           1.10877069   5.        ]
 [ 30.          31.           1.11161487   2.        ]
 [ 35.          96.           1.11559672  11.        ]
 [ 20.          56.           1.12249023   2.        ]
 [ 54.          81.           1.13818119   4.        ]
 [ 41.          98.           1.14363871   6.        ]
 [ 89.         100.           1.15395996   8.        ]
 [103.         105.           1.16173517   8.        ]
 [ 59.          97.           1.16210213   3.        ]
 [ 95.         102.           1.1659596   22.        ]
 [ 99.         101.           1.16654463  12.        ]
 [ 53.         109.           1.17006356  23.        ]
 [  1.         111.           1.19093094  24.        ]
 [  0.         106.           1.19539219   9.        ]
 [104.         110.           1.20338815  16.        ]
 [108.         112.           1.23651064  27.        ]
 [113.         115.           1.24673846  36.        ]
 [107.         114.           1.26252729  24.        ]
 [116.         117.           1.31619005  60.        ]]









##################################################################################################




Different linkage, different hierarchical clustering!

In the video, you saw a hierarchical clustering of the voting countries at the Eurovision song contest using 'complete' linkage. 

Now, perform a hierarchical clustering of the voting countries with 'single' linkage, and compare the resulting dendrogram with the one in the video.

Different linkage, different hierarchical clustering!

You are given an array samples. 

Each row corresponds to a voting country, and each column corresponds to a performance that was voted for.

The list country_names gives the name of each voting country. This dataset was obtained from Eurovision.



Import linkage and dendrogram from scipy.cluster.hierarchy.
Perform hierarchical clustering on samples using the linkage() function with the method='single' keyword argument. Assign the result to mergings.
Plot a dendrogram of the hierarchical clustering, using the list country_names as the labels. In addition, specify the leaf_rotation=90, and leaf_font_size=6 keyword arguments as you have done earlier.
Hint
To import x from y, use the command from y import x, and to import x as the alias y, use the command import x as y.
Pass in samples and method='single' as arguments to the linkage() function to perform hierarchical clustering with single linkage.
To plot the dendrogram, you need to specify the following arguments inside dendrogram(): mergings, labels=country_names, leaf_rotation=90, and leaf_font_size=6





In [1]: # Perform the necessary imports
        import matplotlib.pyplot as plt
        from scipy.cluster.hierarchy import linkage, dendrogram
        
        # Calculate the linkage: mergings
        mergings = linkage(samples, method='single')
        
        # Plot the dendrogram
        dendrogram(mergings,
                   labels=country_names,
                   leaf_rotation=90,
                   leaf_font_size=6,
        )
        plt.show()
        
        
        print(country_names)
        
        print("Mergings: \n", mergings)
['Albania', 'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Belarus', 'Belgium', 'Bosnia & Herzegovina', 'Bulgaria', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Estonia', 'F.Y.R. Macedonia', 'Finland', 'France', 'Georgia', 'Germany', 'Greece', 'Hungary', 'Iceland', 'Ireland', 'Israel', 'Italy', 'Latvia', 'Lithuania', 'Malta', 'Moldova', 'Montenegro', 'Norway', 'Poland', 'Russia', 'San Marino', 'Serbia', 'Slovenia', 'Spain', 'Sweden', 'Switzerland', 'The Netherlands', 'Ukraine', 'United Kingdom']
Mergings: 
 [[10.         19.          6.78232998  2.        ]
 [ 9.         35.          7.21110255  2.        ]
 [22.         30.          8.          2.        ]
 [25.         26.          8.36660027  2.        ]
 [ 5.         40.          8.94427191  2.        ]
 [ 6.         39.          9.2736185   2.        ]
 [12.         21.          9.38083152  2.        ]
 [33.         45.          9.79795897  3.        ]
 [13.         49.          9.89949494  4.        ]
 [ 7.         29.         10.48808848  2.        ]
 [15.         31.         10.58300524  2.        ]
 [44.         50.         10.86278049  6.        ]
 [ 8.         42.         11.04536102  3.        ]
 [37.         52.         11.22497216  3.        ]
 [48.         55.         11.22497216  5.        ]
 [17.         46.         11.22497216  3.        ]
 [53.         56.         11.3137085  11.        ]
 [41.         58.         11.40175425 12.        ]
 [20.         59.         11.66190379 13.        ]
 [43.         51.         12.          4.        ]
 [28.         57.         12.08304597  4.        ]
 [32.         62.         12.08304597  5.        ]
 [14.         34.         13.11487705  2.        ]
 [ 3.         60.         13.41640786 14.        ]
 [61.         64.         13.56465997  6.        ]
 [63.         65.         13.7113092  19.        ]
 [18.         67.         14.07124728 20.        ]
 [ 1.         68.         14.14213562 21.        ]
 [11.         69.         14.14213562 22.        ]
 [54.         70.         14.49137675 25.        ]
 [ 4.         71.         14.62873884 26.        ]
 [16.         23.         14.76482306  2.        ]
 [38.         66.         15.87450787  7.        ]
 [ 0.         72.         15.93737745 27.        ]
 [73.         75.         16.55294536 29.        ]
 [27.         76.         16.91153453 30.        ]
 [74.         77.         17.20465053 37.        ]
 [24.         78.         17.66352173 38.        ]
 [36.         79.         17.72004515 39.        ]
 [47.         80.         18.38477631 41.        ]
 [ 2.         81.         19.79898987 42.        ]]









##################################################################################################









Extracting the cluster labels


In the previous exercise, you saw that the intermediate clustering of the grain samples at height 6 has 3 clusters. 

Now, use the fcluster() function to extract the cluster labels for this intermediate clustering, and compare the labels with the grain varieties using a cross-tabulation.

The hierarchical clustering has already been performed and mergings is the result of the linkage() function. 

The list varieties gives the variety of each grain sample.




Import:
pandas as pd.
fcluster from scipy.cluster.hierarchy.
Perform a flat hierarchical clustering by using the fcluster() function on mergings. Specify a maximum height of 6 and the keyword argument criterion='distance'.
Create a DataFrame df with two columns named 'labels' and 'varieties', using labels and varieties, respectively, for the column values. This has been done for you.
Create a cross-tabulation ct between df['labels'] and df['varieties'] to count the number of times each grain variety coincides with each cluster label.
Hint
To import x from y, use the command from y import x, and to import x as the alias y, use the command import x as y.
Use fcluster() with the arguments mergings, 6, and criterion='distance' to perform the flat hierarchical clustering.
To create the cross-tabulation, use pd.crosstab() with df['labels'] and df['varieties'] as arguments.




In [1]: # Perform the necessary imports
        import pandas as pd
        from scipy.cluster.hierarchy import fcluster
        
        # Use fcluster to extract labels: labels
        labels = fcluster(mergings, 6, criterion='distance')
        
        print("Labels: \n", labels)
        
        
        # Create a DataFrame with labels and varieties as columns: df
        df = pd.DataFrame({'labels': labels, 'varieties': varieties})
        
        # Create crosstab: ct
        ct = pd.crosstab(df['labels'], df['varieties'])
        
        # Display ct
        print(ct)
Labels: 
 [3 3 3 3 3 1 3 3 1 3 3 3 1 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1
 1 1 1 1 1]
varieties  Canadian wheat  Kama wheat  Rosa wheat
labels                                           
1                      14           3           0
2                       0           0          14
3                       0          11           0








##################################################################################################










t-SNE visualization of grain dataset
In the video, you saw t-SNE applied to the iris dataset. In this exercise, you'll apply t-SNE to the grain samples data and inspect the resulting t-SNE features using a scatter plot. You are given an array samples of grain samples and a list variety_numbers giving the variety number of each grain sample.



Import TSNE from sklearn.manifold.




Create a TSNE instance called model with learning_rate=200.
Apply the .fit_transform() method of model to samples. Assign the result to tsne_features.
Select the column 0 of tsne_features. Assign the result to xs.
Select the column 1 of tsne_features. Assign the result to ys.
Make a scatter plot of the t-SNE features xs and ys. To color the points by the grain variety, specify the additional keyword argument c=variety_numbers.
Hint
To import x from y, use the command from y import x.
Use TSNE() with the keyword argument learning_rate=200 to create the desired TSNE instance.
Pass in samples as an argument to model.fit_transform() to fit the model to the data and then transform it.
You can select column 0 of tsne_features using tsne_features[:,0], and column 1 using tsne_features[:,1].
To create the scatter plot, use plt.scatter() with xs, ys, and c=variety_numbers as arguments.






In [1]: # Import TSNE
        from sklearn.manifold import TSNE
        
        # Create a TSNE instance: model
        model = TSNE(learning_rate=200)
        
        print("Model:")
        print(model)
        
        print(dir(model))
        
        
        
        # Apply fit_transform to samples: tsne_features
        tsne_features = model.fit_transform(samples)
        print("tsne_features  :\n", tsne_features)
        
        
        # Select the 0th feature: xs
        xs = tsne_features[:,0]
        
        # Select the 1st feature: ys
        ys = tsne_features[:,1]
        
        # Scatter plot, coloring by variety_numbers
        plt.scatter(xs, ys, c=variety_numbers)
        plt.show()
Model:
TSNE(angle=0.5, early_exaggeration=12.0, init='random', learning_rate=200,
     method='barnes_hut', metric='euclidean', min_grad_norm=1e-07,
     n_components=2, n_iter=1000, n_iter_without_progress=300, perplexity=30.0,
     random_state=None, verbose=0)
['_EXPLORATION_N_ITER', '_N_ITER_CHECK', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_fit', '_get_param_names', '_get_tags', '_tsne', 'angle', 'early_exaggeration', 'fit', 'fit_transform', 'get_params', 'init', 'learning_rate', 'method', 'metric', 'min_grad_norm', 'n_components', 'n_iter', 'n_iter_without_progress', 'perplexity', 'random_state', 'set_params', 'verbose']
tsne_features  :
 [[ 4.21290779e+00 -4.91868854e-01]
 [ 4.61933708e+00  2.16737556e+00]
 [ 6.18396759e-01  7.09419727e-01]
 [-3.84730510e-02  2.24296403e+00]
 [ 6.23100901e+00 -6.88968971e-02]
 [ 1.22902644e+00  8.42335165e-01]
 [ 7.99311876e-01 -2.38034868e+00]
 [ 9.29446425e-03  1.04637301e+00]
 [ 6.92836142e+00 -2.28303242e+00]
 [ 6.58034801e+00 -1.58400965e+00]
 [ 2.68659616e+00 -5.64859343e+00]
 [ 1.62693608e+00  2.62990832e+00]
 [-1.64194000e+00 -1.66726518e+00]
 [-1.14836287e+00  3.19484919e-01]
 [-1.01502264e+00  1.00143456e+00]
 [-5.77027440e-01 -3.07144094e+00]
 [-1.34482403e+01  6.91558695e+00]
 [ 5.45893621e+00  4.14892912e-01]
 [ 2.53344727e+00  1.68601584e+00]
 [-9.19657326e+00  7.57784462e+00]
 [-2.98661459e-02 -5.56513846e-01]
 [ 2.90019006e-01  4.11738187e-01]
 [ 6.22290945e+00  1.04936802e+00]
 [-4.73521423e+00  3.80244374e+00]
 [ 4.17092180e+00  9.73300338e-01]
 [ 6.64301872e+00  3.93694699e-01]
 [-5.20247746e+00  6.58967113e+00]
 [-3.58474088e+00  5.65716648e+00]
 [-8.40235129e-02  4.91071820e-01]
 [-2.27009392e+00  1.84397131e-01]
 [-2.51734090e+00  3.33147931e+00]
 [ 3.28490400e+00 -3.29794955e+00]
 [-6.48009539e-01 -2.20274830e+00]
 [ 7.04711199e-01  2.21599650e+00]
 [ 3.76454568e+00 -2.16645654e-03]
 [ 5.80849934e+00 -1.56969929e+00]
 [ 5.69458485e+00 -3.01864314e+00]
 [ 7.71251583e+00 -5.63120747e+00]
 [ 1.51731730e+00 -1.62356174e+00]
 [-1.55090542e+01  8.71013069e+00]
 [-1.06256688e+00  2.21779227e+00]
 [-7.25565791e-01  2.92332673e+00]
 [-2.68261981e+00  4.91227579e+00]
 [ 3.24532413e+00 -6.26127195e+00]
 [ 2.27834153e+00 -2.18517065e+00]
 [ 1.07051098e+00  3.15853858e+00]
 [ 5.13882875e+00  1.14158177e+00]
 [ 2.24002934e+00 -1.50283754e+00]
 [ 2.02538610e+00 -5.85131586e-01]
 [ 3.33840632e+00  3.68688583e-01]
 [-1.24635935e-01 -2.62909293e+00]
 [ 3.34698486e+00 -7.36189604e+00]
 [ 2.78847218e-01 -3.26640511e+00]
 [ 1.25307098e-01 -1.27682662e+00]
 [ 3.52787304e+00  1.90267062e+00]
 [ 4.06401825e+00  4.79534268e-01]
 [ 1.03273630e+00 -2.13128522e-01]
 [ 4.30324459e+00  2.11618876e+00]
 [ 4.69249153e+00 -4.54727486e-02]
 [-4.28247595e+00  3.75424814e+00]
 [-6.62730026e+00  5.31540251e+00]
 [-6.46700621e+00  4.72103834e+00]
 [-6.67959404e+00  7.09068155e+00]
 [-1.01716919e+01  6.58081484e+00]
 [-3.36504555e+00  3.53258729e+00]
 [-3.30477262e+00  5.09608746e+00]
 [ 3.11312938e+00  2.70296812e+00]
 [ 9.28398430e-01  1.78265059e+00]
 [ 2.94578195e+00  2.38989830e+00]
 [-5.96135283e+00  7.39010906e+00]
 [ 7.98540449e+00 -8.76785851e+00]
 [ 5.90436649e+00 -7.95133162e+00]
 [ 6.61472082e+00 -8.39318848e+00]
 [ 1.31037025e+01 -1.08359480e+01]
 [ 6.44182539e+00 -7.16112804e+00]
 [ 5.42396164e+00 -8.11371708e+00]
 [ 7.62724733e+00 -7.92298269e+00]
 [ 1.14165621e+01 -1.45768929e+01]
 [ 9.30318260e+00 -1.28711338e+01]
 [ 8.08918571e+00 -5.91687059e+00]
 [ 4.56523561e+00 -8.12893581e+00]
 [ 8.65863514e+00 -1.28217793e+01]
 [ 1.03030586e+01 -1.43152723e+01]
 [ 1.55411825e+01 -1.06931505e+01]
 [ 1.34715939e+01 -1.18913288e+01]
 [ 1.29992971e+01 -8.72205257e+00]
 [ 1.50161104e+01 -9.25023556e+00]
 [ 1.14626141e+01 -1.19972219e+01]
 [ 1.03111582e+01 -1.52499752e+01]
 [ 1.07534084e+01 -1.48972740e+01]
 [ 1.50323429e+01 -1.18861399e+01]
 [ 1.20125237e+01 -1.02048235e+01]
 [ 1.18283205e+01 -1.06055050e+01]
 [ 8.17945004e+00 -1.32329998e+01]
 [ 8.77187729e+00 -1.20519257e+01]
 [ 7.11459732e+00 -7.05635691e+00]
 [ 1.23078909e+01 -1.21989479e+01]
 [ 1.46851339e+01 -1.04036341e+01]
 [ 1.18323002e+01 -8.81202507e+00]
 [ 1.39972591e+01 -9.38972569e+00]
 [ 5.43853283e+00 -6.78238583e+00]
 [ 1.35005913e+01 -7.77566385e+00]
 [ 1.06636581e+01 -1.28441525e+01]
 [ 1.23874140e+01 -1.18247166e+01]
 [ 1.18943653e+01 -1.12068281e+01]
 [ 1.33633871e+01 -9.70959759e+00]
 [ 1.27488871e+01 -9.99477291e+00]
 [ 8.41183758e+00 -8.27092934e+00]
 [ 1.31701946e+01 -1.30538673e+01]
 [ 1.44887438e+01 -8.63577652e+00]
 [ 1.36525869e+01 -8.68969154e+00]
 [ 1.18206549e+01 -1.25789080e+01]
 [ 1.44989347e+01 -9.82301140e+00]
 [ 8.64067173e+00 -1.40454836e+01]
 [ 1.11836042e+01 -1.49929371e+01]
 [ 1.41765900e+01 -1.04179993e+01]
 [ 1.01533966e+01 -1.21675444e+01]
 [ 1.28177214e+01 -1.12959232e+01]
 [ 1.10850363e+01 -1.11186333e+01]
 [ 1.36735010e+01 -1.27472630e+01]
 [ 9.71194458e+00 -1.46641340e+01]
 [ 1.00874796e+01 -9.53389072e+00]
 [ 4.85606146e+00 -6.79677677e+00]
 [ 1.18517723e+01 -9.35352707e+00]
 [ 4.08701944e+00 -3.70616269e+00]
 [ 1.01381159e+01 -1.15248995e+01]
 [ 9.62029266e+00 -1.17948742e+01]
 [ 1.29185362e+01 -7.84350491e+00]
 [ 1.53188648e+01 -1.16847248e+01]
 [ 6.08539534e+00 -9.37882328e+00]
 [ 1.22592945e+01 -8.75357151e+00]
 [ 1.28355303e+01 -1.04345932e+01]
 [ 3.42757368e+00 -5.78521299e+00]
 [ 4.86847734e+00 -6.53411579e+00]
 [ 3.52288628e+00 -6.64400578e+00]
 [ 2.62162042e+00 -3.51023889e+00]
 [ 8.12624836e+00 -7.50213003e+00]
 [ 4.87289095e+00 -2.04532146e+00]
 [ 4.72204351e+00 -2.21279502e+00]
 [ 5.47453499e+00 -5.88164330e+00]
 [-1.33517838e+01  8.16888714e+00]
 [-1.54535923e+01  9.57587528e+00]
 [-1.45096664e+01  8.62319851e+00]
 [-1.27274752e+01  1.05421944e+01]
 [-9.78143787e+00  1.03509054e+01]
 [-1.35200539e+01  1.35044947e+01]
 [-5.84918261e+00  4.59647560e+00]
 [-1.00454874e+01  8.43262482e+00]
 [-5.19052458e+00  7.31444311e+00]
 [-1.18770113e+01  1.43752193e+01]
 [-1.19188709e+01  1.13895340e+01]
 [-1.50023384e+01  1.19786243e+01]
 [-1.09117012e+01  9.28233814e+00]
 [-6.41546869e+00  1.11512089e+01]
 [-8.00973606e+00  1.15175285e+01]
 [-1.27486935e+01  1.37819071e+01]
 [-6.56712484e+00  1.06925335e+01]
 [-1.08999271e+01  9.75397205e+00]
 [-9.43208408e+00  1.04008141e+01]
 [-1.21514206e+01  1.24930172e+01]
 [-4.84638166e+00  7.50372696e+00]
 [-9.24004650e+00  9.62028217e+00]
 [-1.16309214e+01  1.04109249e+01]
 [-1.04107275e+01  8.23000145e+00]
 [-1.39348984e+01  1.38510771e+01]
 [-4.97664165e+00  4.77626753e+00]
 [-1.16594658e+01  9.01544762e+00]
 [-6.80344439e+00  8.56223011e+00]
 [-8.83482647e+00  1.18543663e+01]
 [-6.94556570e+00  1.13018866e+01]
 [-1.44783907e+01  1.35190382e+01]
 [-1.44596891e+01  1.28081779e+01]
 [-8.53982162e+00  1.23742714e+01]
 [-1.23508520e+01  1.31440048e+01]
 [-1.09685011e+01  1.40814142e+01]
 [-9.54684734e+00  1.42110748e+01]
 [-1.16287146e+01  1.32859879e+01]
 [-9.44258118e+00  1.37681217e+01]
 [-1.30813560e+01  1.29558430e+01]
 [-4.36974335e+00  4.12664032e+00]
 [-1.06503134e+01  1.28393641e+01]
 [-1.20679960e+01  9.22784901e+00]
 [-1.14245205e+01  9.87554169e+00]
 [-1.16467943e+01  1.21771860e+01]
 [-1.44703827e+01  9.34175968e+00]
 [-8.33152866e+00  1.07240343e+01]
 [-1.09092178e+01  1.08516693e+01]
 [-8.21165371e+00  1.32491655e+01]
 [-1.51866608e+01  1.30209112e+01]
 [-1.00494871e+01  1.47562532e+01]
 [-1.14535646e+01  1.41199837e+01]
 [-7.59314394e+00  1.20625362e+01]
 [-7.02631569e+00  9.68329144e+00]
 [-1.01313438e+01  1.37861290e+01]
 [-8.62069798e+00  9.06860733e+00]
 [-1.19452181e+01  8.13940144e+00]
 [-1.32315502e+01  9.01533127e+00]
 [-1.18537893e+01  6.98899746e+00]
 [-5.50135565e+00  7.52128887e+00]
 [-4.17479801e+00  6.05412579e+00]
 [-1.29228954e+01  9.83736610e+00]
 [-3.88166738e+00  5.05658817e+00]
 [-7.60649347e+00  1.26777172e+01]
 [-1.59221869e+01  1.10000095e+01]
 [-8.18134594e+00  8.17868996e+00]
 [-7.30214310e+00  8.39992142e+00]
 [-8.65320206e+00  1.26710072e+01]
 [-1.59412556e+01  1.05975113e+01]
 [-7.21545362e+00  9.52897263e+00]
 [-1.32648659e+01  1.03891230e+01]]








##################################################################################################







A t-SNE map of the stock market
t-SNE provides great visualizations when the individual samples can be labeled. In this exercise, you'll apply t-SNE to the company stock price data. A scatter plot of the resulting t-SNE features, labeled by the company names, gives you a map of the stock market! The stock price movements for each company are available as the array normalized_movements (these have already been normalized for you). The list companies gives the name of each company. PyPlot (plt) has been imported for you.



Import TSNE from sklearn.manifold.


Create a TSNE instance called model with learning_rate=50.
Apply the .fit_transform() method of model to normalized_movements. Assign the result to tsne_features.
Select column 0 and column 1 of tsne_features.
Make a scatter plot of the t-SNE features xs and ys. Specify the additional keyword argument alpha=0.5.
Code to label each point with its company name has been written for you using plt.annotate(), so just hit 'Submit Answer' to see the visualization!
Hint


To import x from y, use the command from y import x.


Use TSNE() with the keyword argument learning_rate=50 to create the desired TSNE instance.
Pass in normalized_movements as an argument to model.fit_transform() to fit the model to the data and then transform it.
You can select column 0 of tsne_features using tsne_features[:,0], and column 1 using tsne_features[:,1].
To create the scatter plot, use plt.scatter() with xs, ys, and alpha=0.5 as arguments.








##################################################################################################








companies:

In [1]: # Import TSNE
        from sklearn.manifold import TSNE
        
        # Create a TSNE instance: model
        model = TSNE(learning_rate=50)
        
        print(model)
        
        
        # Apply fit_transform to normalized_movements: tsne_features
        tsne_features = model.fit_transform(normalized_movements)
        
        # Select the 0th feature: xs
        xs = tsne_features[:,0]
        
        # Select the 1th feature: ys
        ys = tsne_features[:,1]
        
        # Scatter plot
        plt.scatter(xs, ys, alpha=0.5)
        
        # Annotate the points
        for x, y, company in zip(xs, ys, companies):
            plt.annotate(company, (x, y), fontsize=5, alpha=0.75)
        plt.show()
TSNE(angle=0.5, early_exaggeration=12.0, init='random', learning_rate=50,
     method='barnes_hut', metric='euclidean', min_grad_norm=1e-07,
     n_components=2, n_iter=1000, n_iter_without_progress=300, perplexity=30.0,
     random_state=None, verbose=0)










##################################################################################################








Correlated data in nature

You are given an array grains giving the width and length of samples of grain. 

You suspect that width and length will be correlated. To confirm this, make a scatter plot of width vs length and measure their Pearson correlation.



matplotlib.pyplot as plt.
pearsonr from scipy.stats.
Assign column 0 of grains to width and column 1 of grains to length.
Make a scatter plot with width on the x-axis and length on the y-axis.
Use the pearsonr() function to calculate the Pearson correlation of width and length.
Hint
To import x from y, use the command from y import x, and to import x as the alias y, use the command import x as y.
You can select column 0 of grains using grains[:,0], and column 1 using grains[:,1].
Use plt.scatter() with width and length as arguments to create the scatter plot.
Use pearsonr() with width and length as arguments to calculate the Pearson correlation.





In [1]: # Perform the necessary imports
        import matplotlib.pyplot as plt
        from scipy.stats import pearsonr
        
        # Assign the 0th column of grains: width
        width = grains[:,0]
        
        # Assign the 1st column of grains: length
        length = grains[:,1]
        
        # Scatter plot width vs length
        plt.scatter(width, length)
        plt.axis('equal')
        plt.show()
        
        # Calculate the Pearson correlation
        correlation, pvalue = pearsonr(width, length)
        
        # Display the correlation
        print(correlation)
0.8604149377143466


In [2]: # Perform the necessary imports
        import matplotlib.pyplot as plt
        from scipy.stats import pearsonr
        
        # Assign the 0th column of grains: width
        width = grains[:,0]
        
        # Assign the 1st column of grains: length
        length = grains[:,1]
        
        # Scatter plot width vs length
        plt.scatter(width, length)
        plt.axis('equal')
        plt.show()
        
        # Calculate the Pearson correlation
        correlation, pvalue = pearsonr(width, length)
        
        print(correlation,pvalue)
        
        
        # Display the correlation
        print(correlation)
0.8604149377143466 8.121332906193694e-63
0.8604149377143466








##################################################################################################








Decorrelating the grain measurements with PCA
You observed in the previous exercise that the width and length measurements of the grain are correlated. Now, you'll use PCA to decorrelate these measurements, then plot the decorrelated points and measure their Pearson correlation.


Import PCA from sklearn.decomposition.
Create an instance of PCA called model.
Use the .fit_transform() method of model to apply the PCA transformation to grains. Assign the result to pca_features.
The subsequent code to extract, plot, and compute the Pearson correlation of the first two columns pca_features has been written for you, so hit 'Submit Answer' to see the result!
Hint
To import x from y, use the command from y import x.
To create an instance of the PCA class, use PCA().
Apply the .fit_transform() method on model with grains as the argument to apply the PCA transformation to the data.




In [1]: # Import PCA
        from sklearn.decomposition import PCA
        
        # Create PCA instance: model
        model = PCA()
        
        # Apply the fit_transform method of model to grains: pca_features
        pca_features = model.fit_transform(grains)
        
        # Assign 0th column of pca_features: xs
        xs = pca_features[:,0]
        
        # Assign 1st column of pca_features: ys
        ys = pca_features[:,1]
        
        # Scatter plot xs vs ys
        plt.scatter(xs, ys)
        plt.axis('equal')
        plt.show()
        
        # Calculate the Pearson correlation of xs and ys
        correlation, pvalue = pearsonr(xs, ys)
        
        # Display the correlation
        print(correlation)
2.5478751053409354e-17

In [2]: # Import PCA
        from sklearn.decomposition import PCA
        
        # Create PCA instance: model
        model = PCA()
        print(model)
        
        # Apply the fit_transform method of model to grains: pca_features
        pca_features = model.fit_transform(grains)
        print(pca_features)
        
        
        # Assign 0th column of pca_features: xs
        xs = pca_features[:,0]
        
        # Assign 1st column of pca_features: ys
        ys = pca_features[:,1]
        
        # Scatter plot xs vs ys
        plt.scatter(xs, ys)
        plt.axis('equal')
        plt.show()
        
        # Calculate the Pearson correlation of xs and ys
        correlation, pvalue = pearsonr(xs, ys)
        
        # Display the correlation
        print(correlation)
PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
    svd_solver='auto', tol=0.0, whiten=False)
[[ 1.37546375e-01  4.48701536e-02]
 [-9.77931663e-03 -1.04853394e-01]
 [-2.09502378e-01 -2.76013257e-01]
 [-1.57279093e-01 -2.87226133e-01]
 [ 2.16563481e-01 -2.14516232e-01]
 [-1.52413159e-01 -1.96070646e-01]
 [-5.01506254e-02 -4.21863576e-02]
 [-1.32653965e-01 -1.66650003e-01]
 [ 4.58374510e-01  1.12533346e-01]
 [ 3.53956661e-01 -2.62395364e-02]
 [ 5.51223081e-02  6.73928808e-02]
 [-1.83358870e-01 -7.74647317e-02]
 [-1.83867947e-01 -7.52873846e-02]
 [-1.80584321e-01 -1.66510665e-02]
 [-2.05119162e-01  1.75694184e-02]
 [-1.65911373e-01 -2.34590748e-01]
 [-3.12392996e-01 -4.21318181e-01]
 [ 8.51314987e-02 -2.61320442e-01]
 [-1.93203059e-01 -4.30192803e-01]
 [-4.43556277e-01 -9.60472265e-02]
 [-6.01669339e-02  1.18514214e-01]
 [-1.41381057e-01  3.22563195e-04]
 [ 1.50648029e-01 -1.97778454e-01]
 [-6.13453283e-01 -9.03020125e-02]
 [ 1.14723866e-01  1.13018030e-01]
 [ 2.61046944e-01  5.77271824e-03]
 [-3.28273723e-01  2.96505572e-02]
 [-3.73010742e-01  8.34891973e-02]
 [-9.13571509e-02 -2.70198731e-02]
 [-2.10284878e-01  7.69858755e-02]
 [-3.15489555e-01  1.06582768e-01]
 [ 1.70638550e-01 -4.34273038e-03]
 [ 2.16400636e-02  1.12381094e-01]
 [-1.02891839e-01  5.57083022e-02]
 [ 1.08546684e-01 -2.99347942e-05]
 [ 2.06578055e-01 -1.22699614e-01]
 [ 2.83144391e-01 -3.17732911e-02]
 [ 4.27722090e-01 -1.96376721e-01]
 [ 3.99117613e-02 -5.05461242e-03]
 [-1.52900204e-01 -1.78272815e-01]
 [-2.81339490e-01 -1.00373201e-01]
 [-2.77753919e-01 -9.99941473e-02]
 [-4.14095899e-01 -2.69194811e-01]
 [ 2.78910868e-01  5.31212157e-02]
 [ 9.18927033e-02 -1.88092810e-01]
 [-2.60443134e-01 -8.17092704e-02]
 [ 1.41627844e-01 -5.70530607e-02]
 [ 3.06470699e-02 -1.28469221e-01]
 [-4.35436386e-02 -7.83021121e-02]
 [ 3.76594689e-02  3.20792963e-02]
 [-2.49216065e-02 -3.81247563e-02]
 [ 1.47064622e-01 -1.05842828e-01]
 [-2.65525026e-02  1.67248904e-01]
 [-1.33874924e-01 -3.37458674e-02]
 [-6.55529344e-03  1.83865510e-01]
 [ 2.67198190e-02  8.27973805e-02]
 [-1.09333394e-01 -2.44785469e-01]
 [-9.00413789e-02 -2.74261190e-01]
 [ 1.28248637e-01 -1.01975180e-01]
 [-5.05952251e-01 -1.25791846e-01]
 [-7.38406138e-01 -8.23155216e-02]
 [-8.01399314e-01 -1.72364729e-01]
 [-5.63398494e-01 -1.86528402e-01]
 [-3.00153312e-01 -4.19087375e-03]
 [-4.30567140e-01 -5.53497781e-02]
 [-4.65732997e-01 -2.05487590e-01]
 [-4.27172738e-02  5.37028769e-02]
 [-7.93200743e-02  6.48937211e-02]
 [-1.13280513e-01  4.31753276e-02]
 [-4.07229063e-01  1.51269036e-01]
 [ 6.25867169e-01  1.26893333e-01]
 [ 4.28215627e-01  6.27694858e-02]
 [ 4.83134187e-01 -3.46160968e-02]
 [ 8.33237600e-01 -1.80559923e-01]
 [ 4.44107172e-01  7.33741440e-02]
 [ 3.44209251e-01  5.27730448e-02]
 [ 4.27210651e-01  1.67249101e-01]
 [ 1.08597943e+00  1.80276008e-01]
 [ 8.71074341e-01  2.29233173e-01]
 [ 3.66791581e-01 -9.48854747e-02]
 [ 3.22748740e-01 -2.76474843e-03]
 [ 6.72753013e-01 -2.19000752e-01]
 [ 8.91812152e-01 -4.60756417e-02]
 [ 9.09158147e-01  8.79546402e-02]
 [ 9.13847833e-01  5.41462559e-02]
 [ 6.69541971e-01  4.61684193e-02]
 [ 6.73308316e-01 -9.76224519e-02]
 [ 9.70875286e-01  2.71856774e-01]
 [ 1.22132764e+00  8.00337492e-03]
 [ 1.12608636e+00 -6.98368343e-02]
 [ 1.06898377e+00  2.03858788e-01]
 [ 7.61442386e-01 -6.59935783e-02]
 [ 7.72527402e-01  7.71361615e-02]
 [ 6.98513140e-01 -2.01496014e-01]
 [ 9.42629178e-01  4.88919339e-01]
 [ 5.23240925e-01  1.69034215e-01]
 [ 9.00371650e-01  3.12466384e-02]
 [ 8.18549112e-01  2.98703312e-01]
 [ 6.56081130e-01  2.15708402e-01]
 [ 7.26011958e-01  5.01859583e-02]
 [ 2.39064177e-01 -1.47712649e-01]
 [ 4.79361352e-01 -1.67769263e-01]
 [ 7.77417729e-01 -1.77536343e-01]
 [ 8.39463171e-01  1.48358368e-01]
 [ 7.93692656e-01  1.41121025e-02]
 [ 6.51219720e-01 -1.44580880e-01]
 [ 7.52450920e-01 -8.64668179e-02]
 [ 5.12014870e-01  1.66860098e-02]
 [ 1.12722116e+00  2.80854928e-01]
 [ 6.68858809e-01  1.56965751e-02]
 [ 6.94193656e-01 -8.67687629e-02]
 [ 8.59002054e-01  2.15741735e-02]
 [ 8.37647372e-01 -1.40490559e-01]
 [ 7.90649209e-01  3.49864271e-02]
 [ 1.18679420e+00  3.39155563e-02]
 [ 8.99897783e-01  1.49169390e-01]
 [ 7.32927578e-01 -2.21006177e-01]
 [ 8.29731302e-01 -3.39526042e-02]
 [ 7.86488468e-01 -1.00767311e-02]
 [ 1.04731612e+00  9.22410775e-02]
 [ 9.77517681e-01 -1.02276730e-01]
 [ 5.25621077e-01  4.09938510e-02]
 [ 1.84709771e-01 -1.34532039e-02]
 [ 5.97793181e-01 -1.69472743e-01]
 [ 2.45407414e-03 -4.18433279e-01]
 [ 7.61180176e-01 -1.61124705e-01]
 [ 7.19255080e-01  1.61587685e-01]
 [ 5.43339635e-01 -1.05505476e-01]
 [ 1.00901417e+00  1.69629451e-01]
 [ 4.00661731e-01 -2.27963696e-01]
 [ 5.86798453e-01 -1.57805869e-01]
 [ 7.58440838e-01 -1.06192965e-01]
 [ 2.02489898e-01  1.56042717e-01]
 [ 2.53659818e-01  3.34391306e-02]
 [ 2.08898605e-01 -2.06573923e-02]
 [ 1.53594142e-02 -2.55588407e-01]
 [ 5.98795795e-01  8.74961161e-02]
 [ 2.06531632e-01  2.07507893e-01]
 [ 1.73999285e-01  1.08965281e-01]
 [ 3.23636871e-01 -8.52766638e-03]
 [-2.89502428e-01  1.03473227e-01]
 [-1.85943990e-01  8.68103947e-02]
 [-3.02211651e-01 -1.11019691e-02]
 [-4.97500746e-01 -3.42573057e-02]
 [-5.49708687e-01  1.69395170e-01]
 [-6.34147031e-01  2.16247769e-01]
 [-6.92915536e-01  1.25808492e-01]
 [-4.64428438e-01 -6.77599433e-03]
 [-4.08692365e-01  1.12347849e-01]
 [-6.29845251e-01  2.70529393e-01]
 [-5.48670666e-01  8.83462803e-02]
 [-4.80357555e-01  2.28322417e-01]
 [-4.41621470e-01  1.86399683e-01]
 [-6.75690712e-01  1.73926113e-01]
 [-6.70677050e-01  9.74809485e-02]
 [-6.64120814e-01  2.06943343e-01]
 [-7.04434684e-01 -5.27868863e-02]
 [-5.08630021e-01  2.45135141e-01]
 [-5.12993568e-01  3.28621424e-01]
 [-6.09806143e-01  2.26072289e-01]
 [-3.79150550e-01  1.78501316e-01]
 [-5.00929935e-01  1.67021706e-01]
 [-5.41120470e-01  8.55188173e-02]
 [-4.13027191e-01  3.46354997e-02]
 [-7.71888382e-01 -3.73273087e-02]
 [-6.05643041e-01 -9.03130281e-02]
 [-4.69171038e-01  8.02958596e-02]
 [-4.32142958e-01  1.89075091e-01]
 [-7.25509650e-01  1.65033787e-01]
 [-7.61616552e-01  7.39223632e-02]
 [-5.89819950e-01  2.34878654e-01]
 [-6.19311014e-01  2.31470377e-02]
 [-7.32477986e-01  3.57262380e-02]
 [-6.95560062e-01  6.64030508e-02]
 [-6.64314767e-01  2.50988347e-01]
 [-7.77706378e-01 -7.72666493e-02]
 [-6.90976204e-01  1.39121606e-01]
 [-7.65969083e-01  1.65218888e-01]
 [-6.64914132e-01  9.83690796e-02]
 [-4.42500749e-01  1.07658163e-01]
 [-7.23270536e-01  2.77749574e-02]
 [-4.56316251e-01 -6.50442741e-02]
 [-5.22262586e-01  2.05774575e-02]
 [-6.61493795e-01 -1.84054951e-02]
 [-2.51361380e-01  9.35605837e-02]
 [-5.72097064e-01  2.73008685e-01]
 [-5.12550584e-01  2.79582625e-01]
 [-7.88718810e-01  1.03409100e-01]
 [-6.94641047e-01 -8.24382066e-03]
 [-8.62503908e-01 -1.03522674e-01]
 [-7.94179783e-01  4.42637047e-02]
 [-7.03967505e-01  6.10947819e-03]
 [-5.77207505e-01 -8.22868021e-02]
 [-7.46718966e-01  1.96816879e-01]
 [-4.83158462e-01 -3.27410900e-02]
 [-4.45107703e-01 -1.97450348e-01]
 [-4.41899023e-01 -1.01171044e-01]
 [-3.20769754e-01 -9.67325526e-02]
 [-3.90233402e-01  1.27686256e-01]
 [-4.93487534e-01 -2.75356651e-01]
 [-4.87286157e-01 -5.43735225e-02]
 [-5.74721524e-01 -3.16854106e-01]
 [-7.63201025e-01 -5.09114841e-02]
 [-4.49786373e-01 -1.55831722e-01]
 [-5.17356916e-01 -4.16554470e-02]
 [-5.55467080e-01 -1.00626757e-01]
 [-6.72032359e-01  4.43475020e-02]
 [-3.18909694e-01 -2.30405812e-01]
 [-6.18909928e-01  3.51819507e-02]
 [-4.78413699e-01 -2.74982647e-02]]
2.5478751053409354e-17












##################################################################################################






The first principal component
The first principal component of the data is the direction in which the data varies the most. In this exercise, your job is to use PCA to find the first principal component of the length and width measurements of the grain samples, and represent it as an arrow on the scatter plot.

The array grains gives the length and width of the grain samples. PyPlot (plt) and PCA have already been imported for you.



Make a scatter plot of the grain measurements. This has been done for you.
Create a PCA instance called model.
Fit the model to the grains data.
Extract the coordinates of the mean of the data using the .mean_ attribute of model.
Get the first principal component of model using the .components_[0,:] attribute.
Plot the first principal component as an arrow on the scatter plot, using the plt.arrow() function. You have to specify the first two arguments - mean[0] and mean[1].
Hint
To create an instance of the PCA class, use PCA().
Use model.fit(grains) to fit the model to the data.
To access the .mean_ attribute of model, use model.mean_.
To extract the first principal component of model, use model.components_[0,:].
Inside plt.arrow(), you have to specify mean[0] and mean[1] to plot the first principal component as an arrow.




In [1]: # Make a scatter plot of the untransformed points
        plt.scatter(grains[:,0], grains[:,1])
        
        # Create a PCA instance: model
        model = PCA()
        
        # Fit model to points
        model.fit(grains)
        
        # Get the mean of the grain samples: mean
        mean = model.mean_
        
        # Get the first principal component: first_pc
        first_pc = model.components_[0,:]
        
        # Plot first_pc as an arrow, starting at mean
        plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)
        
        # Keep axes on same scale
        plt.axis('equal')
        plt.show()

In [2]: # Make a scatter plot of the untransformed points
        plt.scatter(grains[:,0], grains[:,1])
        
        # Create a PCA instance: model
        model = PCA()
        
        print(model)
        
        
        # Fit model to points
        model.fit(grains)
        
        print(dir(model))
        
        
        
        # Get the mean of the grain samples: mean
        mean = model.mean_
        
        print("Mean :", mean)
        
        
        
        # Get the first principal component: first_pc
        first_pc = model.components_[0,:]
        
        print("first pc: \n", first_pc)
        
        
        # Plot first_pc as an arrow, starting at mean
        plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)
        
        
        # Keep axes on same scale
        plt.axis('equal')
        plt.show()
PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
    svd_solver='auto', tol=0.0, whiten=False)
['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', '_fit', '_fit_full', '_fit_svd_solver', '_fit_truncated', '_get_param_names', '_get_tags', 'components_', 'copy', 'explained_variance_', 'explained_variance_ratio_', 'fit', 'fit_transform', 'get_covariance', 'get_params', 'get_precision', 'inverse_transform', 'iterated_power', 'mean_', 'n_components', 'n_components_', 'n_features_', 'n_samples_', 'noise_variance_', 'random_state', 'score', 'score_samples', 'set_params', 'singular_values_', 'svd_solver', 'tol', 'transform', 'whiten']
Mean : [3.25860476 5.62853333]
first pc: 
 [0.63910027 0.76912343]










Variance of the PCA features
The fish dataset is 6-dimensional. But what is its intrinsic dimension? Make a plot of the variances of the PCA features to find out. As before, samples is a 2D array, where each row represents a fish. You'll need to standardize the features first.


Create an instance of StandardScaler called scaler.
Create a PCA instance called pca.
Use the make_pipeline() function to create a pipeline chaining scaler and pca.
Use the .fit() method of pipeline to fit it to the fish samples samples.
Extract the number of components used using the .n_components_ attribute of pca. Place this inside a range() function and store the result as features.
Use the plt.bar() function to plot the explained variances, with features on the x-axis and pca.explained_variance_ on the y-axis.
Hint
To create an instances of the classes - in this case, StandardScaler and PCA - you need to use StandardScaler() and PCA().
Pass in scaler and pca as arguments to make_pipeline() to create the pipeline.
Use pipeline.fit() with samples as the argument to fit the pipeline to the data.
Use range(pca.n_components_) to get the number of components.
Inside plt.bar(), specify features and pca.explained_variance_ as arguments.





In [1]: # Perform the necessary imports
        from sklearn.decomposition import PCA
        from sklearn.preprocessing import StandardScaler
        from sklearn.pipeline import make_pipeline
        import matplotlib.pyplot as plt
        
        # Create scaler: scaler
        scaler = StandardScaler()
        
        # Create a PCA instance: pca
        pca = PCA()
        print(pca)
        
        
        # Create pipeline: pipeline
        pipeline = make_pipeline(scaler, pca)
        
        # Fit the pipeline to 'samples'
        pipeline.fit(samples)
        
        # Plot the explained variances
        features = range(pca.n_components_)
        plt.bar(features, pca.explained_variance_)
        plt.xlabel('PCA feature')
        plt.ylabel('variance')
        plt.xticks(features)
        plt.show()
PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
    svd_solver='auto', tol=0.0, whiten=False)







